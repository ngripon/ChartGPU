import type { CartesianSeriesData } from '../config/types';
import { getPointCount, packXYInto } from './cartesianData';

export interface DataStore {
  setSeries(
    index: number,
    data: CartesianSeriesData,
    options?: Readonly<{ xOffset?: number }>
  ): void;
  /**
   * Appends new points to an existing series without re-uploading the entire buffer when possible.
   *
   * - Reuses the same geometric growth policy as `setSeries`.
   * - When no reallocation is needed, writes only the appended byte range via `queue.writeBuffer(...)`.
   * - Maintains `pointCount` for render path queries.
   *
   * Throws if the series has not been set yet.
   */
  appendSeries(index: number, newPoints: CartesianSeriesData): void;
  removeSeries(index: number): void;
  getSeriesBuffer(index: number): GPUBuffer;
  /**
   * Returns the number of points last set for the given series index.
   *
   * Throws if the series has not been set yet.
   */
  getSeriesPointCount(index: number): number;
  dispose(): void;
}

type SeriesEntry = {
  readonly buffer: GPUBuffer;
  readonly capacityBytes: number;
  readonly pointCount: number;
  readonly hash32: number;
  /**
   * X-origin subtracted during packing to preserve Float32 precision for large-magnitude domains
   * (e.g. epoch-ms time axes). Stored so appendSeries can pack consistently.
   */
  readonly xOffset: number;
  /**
   * Growable staging buffer for interleaved Float32 x,y data.
   * Maintained to enable efficient incremental append without repacking all data.
   */
  readonly stagingBuffer: Float32Array;
};

const MIN_BUFFER_BYTES = 4;

function roundUpToMultipleOf4(bytes: number): number {
  return (bytes + 3) & ~3;
}

function nextPow2(bytes: number): number {
  if (!Number.isFinite(bytes) || bytes <= 0) return 1;
  const n = Math.ceil(bytes);
  return 2 ** Math.ceil(Math.log2(n));
}

function computeGrownCapacityBytes(currentCapacityBytes: number, requiredBytes: number): number {
  // Grow geometrically to reduce buffer churn (power-of-two policy).
  // Enforce 4-byte alignment via MIN_BUFFER_BYTES (>= 4) and power-of-two growth.
  const required = Math.max(MIN_BUFFER_BYTES, roundUpToMultipleOf4(requiredBytes));
  const grown = Math.max(MIN_BUFFER_BYTES, nextPow2(required));
  return Math.max(currentCapacityBytes, grown);
}

function fnv1aUpdate(hash: number, words: Uint32Array): number {
  let h = hash >>> 0;
  for (let i = 0; i < words.length; i++) {
    h ^= words[i]!;
    h = Math.imul(h, 0x01000193) >>> 0; // FNV prime
  }
  return h >>> 0;
}

/**
 * Computes a stable 32-bit hash of the Float32 contents using their IEEE-754
 * bit patterns (not numeric equality), to cheaply detect changes.
 */
function hashFloat32ArrayBits(data: Float32Array): number {
  const u32 = new Uint32Array(data.buffer, data.byteOffset, data.byteLength / 4);
  return fnv1aUpdate(0x811c9dc5, u32); // FNV-1a offset basis
}

export function createDataStore(device: GPUDevice): DataStore {
  const series = new Map<number, SeriesEntry>();
  let disposed = false;

  /**
   * Packs CartesianSeriesData into an interleaved Float32Array using packXYInto.
   * Returns a view-safe Float32Array suitable for GPU upload.
   */
  const packCartesianData = (data: CartesianSeriesData, xOffset: number): Float32Array => {
    const pointCount = getPointCount(data);
    if (pointCount === 0) return new Float32Array(0);

    const buffer = new ArrayBuffer(pointCount * 2 * 4);
    const f32 = new Float32Array(buffer);

    packXYInto(f32, 0, data, 0, pointCount, xOffset);

    return f32;
  };

  const assertNotDisposed = (): void => {
    if (disposed) {
      throw new Error('DataStore is disposed.');
    }
  };

  const getSeriesEntry = (index: number): SeriesEntry => {
    assertNotDisposed();
    const entry = series.get(index);
    if (!entry) {
      throw new Error(`Series ${index} has no data. Call setSeries(${index}, data) first.`);
    }
    return entry;
  };

  const setSeries = (index: number, data: CartesianSeriesData, options?: Readonly<{ xOffset?: number }>): void => {
    assertNotDisposed();

    const xOffset = options?.xOffset ?? 0;
    const pointCount = getPointCount(data);
    const packed = packCartesianData(data, xOffset);
    const hash32 = hashFloat32ArrayBits(packed);

    const requiredBytes = roundUpToMultipleOf4(packed.byteLength);
    const targetBytes = Math.max(MIN_BUFFER_BYTES, requiredBytes);

    const existing = series.get(index);
    const unchanged = existing && existing.pointCount === pointCount && existing.hash32 === hash32;
    if (unchanged) return;

    let buffer = existing?.buffer ?? null;
    let capacityBytes = existing?.capacityBytes ?? 0;

    if (!buffer || targetBytes > capacityBytes) {
      const maxBufferSize = device.limits.maxBufferSize;
      if (targetBytes > maxBufferSize) {
        throw new Error(
          `DataStore.setSeries(${index}): required buffer size ${targetBytes} exceeds device.limits.maxBufferSize (${maxBufferSize}).`
        );
      }

      if (buffer) {
        try {
          buffer.destroy();
        } catch {
          // Ignore destroy errors; we are replacing the buffer anyway.
        }
      }

      const grownCapacityBytes = computeGrownCapacityBytes(capacityBytes, targetBytes);
      if (grownCapacityBytes > maxBufferSize) {
        // If geometric growth would exceed the limit, fall back to the exact required size.
        // (Still no shrink: if current capacity was already larger, we'd keep it above.)
        // NOTE: targetBytes is already checked against maxBufferSize above.
        capacityBytes = targetBytes;
      } else {
        capacityBytes = grownCapacityBytes;
      }

      buffer = device.createBuffer({
        size: capacityBytes,
        usage: GPUBufferUsage.VERTEX | GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,
      });
    }

    // View-safe GPU upload: explicitly pass byteOffset and byteLength
    if (packed.byteLength > 0) {
      device.queue.writeBuffer(buffer, 0, packed.buffer, packed.byteOffset, packed.byteLength);
    }

    // Create staging buffer matching the packed data for efficient append
    const stagingBuffer = new Float32Array(capacityBytes / 4);
    stagingBuffer.set(packed);

    series.set(index, {
      buffer,
      capacityBytes,
      pointCount,
      hash32,
      xOffset,
      stagingBuffer,
    });
  };

  const appendSeries = (index: number, newPoints: CartesianSeriesData): void => {
    assertNotDisposed();
    const newPointCount = getPointCount(newPoints);
    if (newPointCount === 0) return;

    const existing = getSeriesEntry(index);
    const prevPointCount = existing.pointCount;
    const nextPointCount = prevPointCount + newPointCount;

    // Each point is 2 floats (x, y) = 8 bytes.
    const requiredBytes = roundUpToMultipleOf4(nextPointCount * 2 * 4);
    const targetBytes = Math.max(MIN_BUFFER_BYTES, requiredBytes);

    let buffer = existing.buffer;
    let capacityBytes = existing.capacityBytes;
    let stagingBuffer = existing.stagingBuffer;

    const maxBufferSize = device.limits.maxBufferSize;

    if (targetBytes > capacityBytes) {
      if (targetBytes > maxBufferSize) {
        throw new Error(
          `DataStore.appendSeries(${index}): required buffer size ${targetBytes} exceeds device.limits.maxBufferSize (${maxBufferSize}).`
        );
      }

      // Replace buffer (no shrink). This is the slow path; we re-upload the full series.
      try {
        buffer.destroy();
      } catch {
        // Ignore destroy errors; we are replacing the buffer anyway.
      }

      const grownCapacityBytes = computeGrownCapacityBytes(capacityBytes, targetBytes);
      capacityBytes = grownCapacityBytes > maxBufferSize ? targetBytes : grownCapacityBytes;

      buffer = device.createBuffer({
        size: capacityBytes,
        usage: GPUBufferUsage.VERTEX | GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,
      });

      // Create new staging buffer with grown capacity
      const newStagingBuffer = new Float32Array(capacityBytes / 4);
      // Copy old data
      newStagingBuffer.set(stagingBuffer.subarray(0, prevPointCount * 2));
      // Pack new data directly into staging buffer
      packXYInto(newStagingBuffer, prevPointCount * 2, newPoints, 0, newPointCount, existing.xOffset);
      
      const fullPacked = newStagingBuffer.subarray(0, nextPointCount * 2);
      if (fullPacked.byteLength > 0) {
        device.queue.writeBuffer(buffer, 0, fullPacked.buffer, fullPacked.byteOffset, fullPacked.byteLength);
      }

      series.set(index, {
        buffer,
        capacityBytes,
        pointCount: nextPointCount,
        hash32: hashFloat32ArrayBits(fullPacked),
        xOffset: existing.xOffset,
        stagingBuffer: newStagingBuffer,
      });
      return;
    }

    // Fast path: pack directly into existing staging buffer and upload only the appended range.
    packXYInto(stagingBuffer, prevPointCount * 2, newPoints, 0, newPointCount, existing.xOffset);
    
    const appendedView = stagingBuffer.subarray(prevPointCount * 2, nextPointCount * 2);
    if (appendedView.byteLength > 0) {
      const byteOffset = prevPointCount * 2 * 4;
      device.queue.writeBuffer(buffer, byteOffset, appendedView.buffer, appendedView.byteOffset, appendedView.byteLength);
    }

    // Incremental FNV-1a update over the appended IEEE-754 bit patterns.
    const appendWords = new Uint32Array(appendedView.buffer, appendedView.byteOffset, appendedView.byteLength / 4);
    const nextHash32 = fnv1aUpdate(existing.hash32, appendWords);

    series.set(index, {
      buffer,
      capacityBytes,
      pointCount: nextPointCount,
      hash32: nextHash32,
      xOffset: existing.xOffset,
      stagingBuffer,
    });
  };

  const removeSeries = (index: number): void => {
    assertNotDisposed();

    const entry = series.get(index);
    if (!entry) return;

    try {
      entry.buffer.destroy();
    } catch {
      // Ignore destroy errors; removal should be best-effort.
    }
    series.delete(index);
  };

  const getSeriesBuffer = (index: number): GPUBuffer => {
    return getSeriesEntry(index).buffer;
  };

  const getSeriesPointCount = (index: number): number => {
    return getSeriesEntry(index).pointCount;
  };

  const dispose = (): void => {
    if (disposed) return;
    disposed = true;

    for (const entry of series.values()) {
      try {
        entry.buffer.destroy();
      } catch {
        // Ignore destroy errors; disposal should be best-effort.
      }
    }
    series.clear();
  };

  return {
    setSeries,
    appendSeries,
    removeSeries,
    getSeriesBuffer,
    getSeriesPointCount,
    dispose,
  };
}

